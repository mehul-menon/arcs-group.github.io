# When making a new entry, copy the commented out entry, paste it, uncomment, and fill out the fields.
# If any of the fields are confusing look at previous examples for guidance.

#- key: null
#  title: null
#  site: null  # project page
#  authors: []
#  equal_contributions: []
#  venue: arXiv
#  year: null
#  thumbnail: null  # Save to /files/[your-folder]/
#  eprint: null   # Example: arXiv:2203.02475 (copy from the "cited as" line on the arXiv page)
#  tags: [mapf, warehouse, arm, traffic]
#  links:     # You can add additional links not listed below
#    arXiv: null
#  abstract: null

- key: Yan25
  title: "Advancing MAPF towards the Real World: A Scalable Multi-Agent Realistic Testbed (SMART)"
  site: https://jingtianyan.github.io/publication/2025-03-02-smart-testbed
  authors: [Jingtian Yan, Zhifei Li, William Kang, Kevin Zheng, Yulun Zhang, Zhe Chen, Yue Zhang, Daniel Harabor, Stephen F. Smith, Jiaoyang Li]
  venue: arXiv
  year: 2025
  thumbnail: /files/jiaoyangli/thumbnails/Yan25.png
  eprint: arXiv:2503.04798
  tags: [mapf, warehouse, execution]
  links:
    arXiv: https://arxiv.org/abs/2503.04798
    Code: https://github.com/JingtianYan/SMART/
    Demo: https://smart-mapf.github.io/demo/
    Video: https://youtu.be/irtFxMjyJXs
  abstract: "We present Scalable Multi-Agent Realistic Testbed (SMART), a realistic and efficient software tool for evaluating Multi-Agent Path Finding (MAPF) algorithms. MAPF focuses on planning collision-free paths for a group of agents. While state-of-the-art MAPF algorithms can plan paths for hundreds of robots in seconds, they often rely on simplified robot models, making their real-world performance unclear. Researchers typically lack access to hundreds of physical robots in laboratory settings to evaluate the algorithms. Meanwhile, industrial professionals who lack expertise in MAPF require an easy-to-use simulator to efficiently test and understand the performance of MAPF algorithms in their specific settings. SMART fills this gap with several advantages: (1) SMART uses a physics-engine-based simulator to create realistic simulation environments, accounting for complex real-world factors such as robot kinodynamics and execution uncertainties, (2) SMART uses an execution monitor framework based on the Action Dependency Graph, facilitating seamless integration with various MAPF algorithms and robot models, and (3) SMART scales to thousands of robots. In addition, we use SMART to explore and demonstrate research questions about the execution of MAPF algorithms in real-world scenarios. "


- key: Qian2024
  title: A Quality Diversity Method to Automatically Generate Multi-Agent Path Finding Benchmark Maps
  site: https://airtclick.github.io/Benchmark.github.io/  # project page
  authors: [Cheng Qian, Yulun Zhang, Varun Bhatt, Matthew C. Fontaine, Stefanos Nikolaidis, Jiaoyang Li]
  equal_contributions: [Cheng Qian, Yulun Zhang]
  venue: arXiv
  year: 2024
  thumbnail: /files/yulunzhang/thumbnails/qd-mapf-bench.png  # Save to /files/[your-folder]/
  eprint: arXiv:2409.06888   # Example: arXiv:2203.02475 (copy from the "cited as" line on the arXiv page)
  tags: [mapf]
  links:     # You can add additional links not listed below
    arXiv: https://arxiv.org/abs/2409.06888
    Code: https://github.com/AirTClick/warehouse-
  abstract: We use the Quality Diversity (QD) algorithm with Neural Cellular Automata (NCA) to generate benchmark maps for Multi-Agent Path Finding (MAPF) algorithms. Previously, MAPF algorithms are tested using fixed, human-designed benchmark maps. However, such fixed benchmark maps have several problems. First, these maps may not cover all the potential failure scenarios for the algorithms. Second, when comparing different algorithms, fixed benchmark maps may introduce bias leading to unfair comparisons between algorithms. Third, since researchers test new algorithms on a small set of fixed benchmark maps, the design of the algorithms may overfit to the small set of maps. In this work, we take advantage of the QD algorithm to (1) generate maps with patterns to comprehensively understand the performance of MAPF algorithms, (2) be able to make fair comparisons between two MAPF algorithms, providing further information on the selection between two algorithms and on the design of the algorithms. Empirically, we employ this technique to generate diverse benchmark maps to evaluate and compare the behavior of different types of MAPF algorithms, including search-based, priority-based, rule-based, and learning-based algorithms. Through both single-algorithm experiments and comparisons between algorithms, we identify patterns where each algorithm excels and detect disparities in runtime or success rates between different algorithms.

